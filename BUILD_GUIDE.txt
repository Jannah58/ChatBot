LARAVEL CHATBOT PROJECT - BUILD GUIDE
=====================================

This guide explains how to build a simple Laravel frontend with AI chatbot integration using Docker.

PROJECT OVERVIEW
================
- Laravel web application (simplified, no database)
- AI chatbot powered by Ollama (Gemma 2B model)
- FastAPI backend for chatbot processing
- All services containerized with Docker
- Simple file structure with no long names

DIRECTORY STRUCTURE
==================
workspace/
├── laravel-blog/          # Laravel application
├── chatbot/               # Python FastAPI chatbot service
├── ollama/                # Ollama AI service
├── gamma/                 # Additional chatbot (simplified name)
├── docker-compose.yml     # Main orchestration file
└── BUILD_GUIDE.txt        # This guide

KEY COMPONENTS
==============

1. LARAVEL APPLICATION (laravel-blog/)
   - No database dependencies (simplified)
   - Static blog posts defined in controller
   - Integrated chatbot UI
   - Uses PHP-FPM + Nginx in Alpine Linux

2. CHATBOT SERVICE (chatbot/)
   - Python FastAPI application
   - Integrates with Ollama for AI processing
   - Serves both API and web interface
   - Requirements: fastapi, uvicorn, requests, langchain

3. OLLAMA SERVICE (ollama/)
   - AI model service (Gemma 2B)
   - Automatically pulls model on startup
   - Provides LLM inference capabilities

4. DOCKER ORCHESTRATION
   - No external databases (simplified)
   - Container networking for service communication
   - Volume mounts for development

BUILD INSTRUCTIONS
==================

PREREQUISITES:
- Docker and Docker Compose installed
- 4GB+ RAM (for AI model)
- Internet connection (for downloading models)

STEPS:

1. Clone/Download the project
   cd /workspace

2. Build all containers:
   sudo docker compose build

3. Start the services:
   sudo docker compose up -d

4. Wait for Ollama to download the model (first run only):
   sudo docker logs ollama -f
   # Wait until you see "model pulled successfully"

5. Access the application:
   - Laravel app: http://localhost
   - Chatbot API: http://localhost:8000
   - Ollama API: http://localhost:11434

DEVELOPMENT WORKFLOW
===================

FOR LARAVEL CHANGES:
- Edit files in laravel-blog/
- Changes are reflected immediately (volume mounted)
- For major changes, rebuild: sudo docker compose build laravel

FOR CHATBOT CHANGES:
- Edit files in chatbot/
- Changes are reflected immediately (volume mounted)
- For dependency changes, rebuild: sudo docker compose build chatbot

FOR OLLAMA MODEL CHANGES:
- Edit ollama/entrypoint.sh
- Rebuild: sudo docker compose build ollama

SIMPLIFIED FEATURES
==================

1. NO DATABASE:
   - Blog posts are static data in controller
   - No migrations, no SQL complexity
   - Faster development and deployment

2. SHORT FILENAMES:
   - Renamed Gamma_Chatbot to gamma
   - Simplified migration names
   - Clean project structure

3. MINIMAL DEPENDENCIES:
   - Removed MySQL, Redis, PHPMyAdmin
   - Only essential services running
   - Reduced resource usage

TROUBLESHOOTING
===============

Common Issues:

1. Docker permission denied:
   sudo usermod -aG docker $USER
   newgrp docker

2. Port conflicts:
   Change ports in docker-compose.yml:
   - 80:80 (Laravel)
   - 8000:8000 (Chatbot)
   - 11434:11434 (Ollama)

3. Model download fails:
   Check internet connection
   Ensure sufficient disk space (4GB+)

4. Laravel key generation error:
   Run manually: sudo docker exec laravel-app php artisan key:generate

5. Permission issues:
   sudo docker exec laravel-app chown -R www-data:www-data storage bootstrap/cache

ARCHITECTURE OVERVIEW
=====================

Request Flow:
1. User visits Laravel app (port 80)
2. Frontend loads with chatbot widget
3. Chat messages sent to FastAPI (port 8000)
4. FastAPI forwards to Ollama (port 11434)
5. AI response returned through the chain

Network Communication:
- All services connected via app-network
- Internal service discovery by container names
- External access only through exposed ports

Security Considerations:
- Development setup (not production-ready)
- No authentication implemented
- All services accessible externally

CUSTOMIZATION GUIDE
===================

Adding New Blog Posts:
1. Edit laravel-blog/app/Http/Controllers/BlogController.php
2. Add new objects to the $posts collection
3. No database changes needed

Changing AI Model:
1. Edit ollama/entrypoint.sh
2. Replace 'gemma2:2b' with desired model
3. Rebuild ollama service

Adding New Routes:
1. Edit laravel-blog/routes/web.php
2. Create corresponding controller methods
3. Add blade templates if needed

Styling Changes:
1. Edit laravel-blog/resources/views/layouts/app.blade.php
2. Modify CSS in the layout file
3. Changes reflect immediately

PERFORMANCE OPTIMIZATION
========================

For Production:
1. Use multi-stage Docker builds
2. Enable PHP OPcache
3. Use production-optimized Nginx config
4. Implement caching strategies
5. Use environment-specific configs

For Development:
1. Use bind mounts for hot reloading
2. Enable debug mode
3. Use development dependencies
4. Implement file watching

SCALING CONSIDERATIONS
=====================

Horizontal Scaling:
- Laravel: Multiple app containers behind load balancer
- Chatbot: Scale FastAPI instances
- Ollama: Consider model sharding for high load

Vertical Scaling:
- Increase container resources
- Optimize AI model size vs performance
- Use faster storage for model files

FUTURE ENHANCEMENTS
==================

Possible Additions:
1. User authentication system
2. Chat history persistence
3. Multiple AI model support
4. Real-time chat with WebSockets
5. Admin panel for content management
6. API rate limiting
7. Monitoring and logging
8. CI/CD pipeline

CONCLUSION
==========

This setup provides a clean, simple foundation for a Laravel application with AI chatbot integration. The containerized approach ensures consistent environments and easy deployment while keeping complexity minimal.

For questions or issues, check the Docker logs:
sudo docker compose logs [service-name]

Happy coding!